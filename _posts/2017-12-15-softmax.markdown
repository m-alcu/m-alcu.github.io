---
layout: post
title: Derivative of $softMax(x)$
date: 2017-12-15 06:15
comments: true
external-url:
categories: softmax algebra
---

Softmax extends binary classification to 'n' levels useful for:  
* Faces  
* Car  
* MNIST Digits 0-9  

Softmax is a generalization of Logistic function. Compress a K-dimension z Real values to a K-dimension $\sigma (z)$

Softmax for K=2 is the same as a sigmoid where $w = w_1 - w_0$ 

Softmax is a generalization of sigmoid for K>2.

Softmax for K Classes:  

$$a_1=w_1^\intercal x \text{   } a_2=w_2^\intercal x \text{   } ...\text{   } a_n=w_n^\intercal x$$

$$P(Y=k|X) = {e^{a_k} \over Z}$$

$$Z = e^{a_1}+e^{a_2}+...+e^{a_K}$$

$$W = [ w_1 w_2...w_K] \text{ (a D x K matrix)}$$

$$A_{N\times K} = X_{N\times D}W_{D\times K}\rightarrow Y_{N\times K}=softmax(A_{N\times K})$$

SoftMax derivative:

Softmax is calculated not from an scalar, but a vector x. $S(a): \mathbb{R}^N \rightarrow \mathbb{R}^N$

$$S(a) = {\begin{pmatrix}
a_1 \\
a_2\\
\vdots\\
a_N
\end{pmatrix}} \rightarrow {\begin{pmatrix}
S_1 \\
S_2\\
\vdots\\
S_N
\end{pmatrix}}$$

$$S_j = {e^{a_i} \over \sum_{k=1}^N e^{a_k}} \forall j \in 1..N$$

$$\dfrac{\partial S_i}{\partial a_j}= \dfrac{\partial {e^{a_i} \over \sum_{k=1}^N e^{a_k}}}{\partial a_j} = D_jS_i $$

$$\begin{pmatrix}
 D_1S_1 & D_2S_1 & D_3S_1 & \cdots & D_NS_1 \\
 D_1S_2 & D_2S_2 & D_3S_2 & \cdots & D_NS_2 \\
 \vdots  & \vdots& \vdots & \ddots & \vdots \\
 D_1S_N & D_2S_N & D_3S_N & \cdots & D_NS_N    
 \end{pmatrix}$$



